# Clowder Installation, Development, and Use
Clowder is an open-source data management platform for research. See the [Clowder website](https://clowder.ncsa.illinois.edu/) for more information about the software and its applications

TERRAREF uses Clowder to organize, annotate, and process  data generated by phenotyping platforms. There are two primary instances of Clowder for this project:

**You can request an account for the [TERRAREF Clowder Interface](http://terraref.ncsa.illinois.edu/clowder/) where this is being tested by clicking **Sign up** in the upper-right corner.** Once access is granted, you can explore collections and datasets. 

Below is a demo that shows how to set up and use Clowder for the Lemnatec Indoor Pipeline:

<iframe width="640" height="360" src="https://www.youtube.com/embed/dCNYEl3ld0s?rel=0&amp;showinfo=0" frameborder="0" allowfullscreen></iframe>


## Installation
 [The NCSA Clowder wiki](https://opensource.ncsa.illinois.edu/confluence/display/CATS/Administrating+Clowder) provides an up-to-date installation guide for Clowder.

In a fresh installation, Clowder is not configured with an email server - it does not send confirmation  when someone registers for an account the confirmation email will not be sent correctly. However the Clowder console will still display the contents of the email, so the confirmation URL can be copied from there. 


## Interface
Clowder consists of three organizational levels:
* **Datasets** consist of one or more files with associated metadata.
* **Collections** consist of one or more datasets.
* **Spaces** consist of collections and datasets. Spaces allow for particular roles to be assigned to particular users.
 
Datasets offer a **Metadata** tab that displays associated information; for example, the contents of .json files originally packaged with the data.  

After selecting a dataset, the **Analysis Environment Instances** menu on the lower right sidebar allows users to launch analysis tools. Currently, users can choose between launching Rstudio or Jupyter. These tools support R and Python as well as many familiar programming languages.  Additional tools can be added based on user demand. 

**Searching the database**  
Clowder allows users to search metadata and filter datasets and files with particular attributes. In development is the ability to query BETYdb based on a particular set of resulting images.



## Uploading data

**USING WEB INTERFACE**  
1. Log in with your account
2. Click 'Datasets' > 'Create'
3. Provide a name and description
4. Click 'Select Files' to choose which files to add
5. Click 'Upload' to save selected files to dataset
6. Click 'View Dataset' to confirm. You can add more content with 'Add Files'.
7. Add metadata, terms of use, etc. 

Some metadata may automatically be generated depending on the types of files uploaded. Metadata can be manually added to files or datasets at any time.
  
**USING API**  
Clowder also includes a RESTful API that allows programmatic interactions such as creating new datasets and downloading files. For example, one can request a list of datasets using:
    ```GET _clowder home URL_/api/datasets```
The current API schema for a Clowder instance can be accessed by selecting **API** from the **?** Help menu in the upper-right corner of the application.

Two example sources that will be pushing high data volumes into Clowder:
* LemnaTec indoor system at Danforth (running)
* LemnaTec outdoor system at Maricopa (in progress)

For typical workflows, the following steps are sufficient to push data into Clowder in an organized fashion:

1. Create a collection to hold relevant datasets (optional)
    ```POST /api/collections``` _provide a name; returns collection ID_  
    
2. Create a dataset to hold relevant files and add it to the collection
    ```POST /api/datasets/createempty``` _provide a name; returns dataset ID_  
    ```POST /api/collections/<collection id>/datasets/<dataset id>```  
    
3. Upload files and metadata to dataset
    ```POST /api/datasets/uploadToDataset/<dataset id>``` _provide file(s) and metadata_  

An extensive API reference can be found [here](https://terraref.ncsa.illinois.edu/clowder/assets/docs/api/index.html).

**USING GLOBUS**  
Some files, e.g. those transferred via Globus, will be moved to the server without triggering Clowder's normal upload paths. These must be transmitted in a certain way to ensure proper handling.

1. Log into Globus and click 'Transfer Files'.
2. Select your source endpoint, and [Terraref](https://www.globus.org/app/endpoints/c50eec62-ea1a-11e5-97d6-22000b9da45e/overview) as the destination. You need to contact NCSA to ensure you have the necessary credentials and folder space to utilize Globus - unrecognized Globus accounts will not be trusted.
3. Transfer your files. You will receive a **Task ID** when the transfer starts.
4. Send this Task ID and requisite information about the transfer to the **TERRAREF Globus Monitor API** as a JSON object:
  ```json
    { "user": <globus_username>
      "globus_id": <Task ID>
      "contents": {
        <dataset1>: {
          <filename1>: {
            "name": <filename1>,
            "md": <file_metadata1>
          },
          <filename2>: {"name": ..., "md": {...}},
          <filename3>: {...},
          ...
        },
        <dataset2>: {
          <filename4>: {...},
          ...
        },
        ...
        }
      }
    }
  ```
  In addition to username and Task ID, you must also send a "contents" object containing each dataset that should be created in Clowder, and the files that belong to that dataset. This allows Clowder to verify it has handled every file in the Globus task.
5. The JSON object is sent to the API via an HTTP request:
 ```POST 141.142.168.72:5454/tasks```
 For example, with cURL this would be done with:
 ```curl -X POST -u <globus_username>:<globus_password> -d <json_object> 141.142.168.72:5454/tasks```

In this way Clowder indexes a pointer to the file on disk rather than making a new copy of the file; thus the file will still be accessible via Globus, FTP, or other methods directed at the filesystem.


## Extractors
Extractors are services that run silently alongside Clowder. They can be configured to wait for specific file types to be uploaded into Clowder, and automatically execute operations on those files to extract metadata. 

**Source code**  
The source code is available as a collection of Git repositorie. Install [Git](https://git-scm.com/) in order to clone them.

**Examples** (from the [git repository](https://opensource.ncsa.illinois.edu/bitbucket/projects/CATS))  
* **extractors-core** includes basic extractors, such as image thumbnail extraction.
* **extractors-dbpedia** uses named-entity recognition and [DBpedia](http://wiki.dbpedia.org/) to extract information from text files
* **extractors-plantcv** invokes appropriate [PlantCV](http://plantcv.danforthcenter.org/) image analysis tools to generate output images and data from uploaded images ([read more about this extractor here](http://opensource.ncsa.illinois.edu/bitbucket/projects/CATS/repos/extractors-plantcv/browse))

**Developing new extractors**  
It is possible to develop extractors for new file types or tasks. 
* [pyClowder](https://opensource.ncsa.illinois.edu/bitbucket/projects/CATS/repos/pyclowder/browse) is designed for this purpose.  
* [Development in Windows](https://opensource.ncsa.illinois.edu/confluence/display/CATS/Deploying+Windows+Extractors)  

More information coming soon.
