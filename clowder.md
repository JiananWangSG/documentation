# Clowder Use, Installation, and Development

Clowder is an open-source data management platform for research. See the [Clowder website](https://clowder.ncsa.illinois.edu/) for more information about the software and its applications

TERRAREF uses Clowder to organize, annotate, and process  data generated by phenotyping platforms. There are two primary instances of Clowder for this project:

**You can request an account for the [TERRAREF Clowder Interface](http://terraref.ncsa.illinois.edu/clowder/) where this is being tested by clicking **Sign up** in the upper-right corner.** Once access is granted, you can explore collections and datasets. 

## Interface

Clowder consists of three organizational levels:
* **Datasets** consist of one or more files with associated metadata.
* **Collections** consist of one or more datasets.
* **Spaces** consist of collections and datasets. Spaces allow for particular roles to be assigned to particular users.
 
Datasets offer a **Metadata** tab that displays associated information; for example, the contents of .json files originally packaged with the data.  

After selecting a dataset, the **Analysis Environment Instances** menu on the lower right sidebar allows users to launch analysis tools. Currently, users can choose between launching Rstudio or Jupyter. These tools support R and Python as well as many familiar programming languages.  Additional tools can be added based on user demand. 

**Searching the database**  
Clowder allows users to search metadata and filter datasets and files with particular attributes. In development is the ability to query BETYdb based on a particular set of resulting images.

## Uploading data

### Web Interface Data Uploads

1. Log in with your account
2. Click 'Datasets' > 'Create'
3. Provide a name and description
4. Click 'Select Files' to choose which files to add
5. Click 'Upload' to save selected files to dataset
6. Click 'View Dataset' to confirm. You can add more content with 'Add Files'.
7. Add metadata, terms of use, etc. 

Some metadata may automatically be generated depending on the types of files uploaded. Metadata can be manually added to files or datasets at any time.
  
### API Data Uploads

Clowder also includes a RESTful API that allows programmatic interactions such as creating new datasets and downloading files. For example, one can request a list of datasets using:
    ```GET _clowder home URL_/api/datasets```
The current API schema for a Clowder instance can be accessed by selecting **API** from the **?** Help menu in the upper-right corner of the application.

Two example sources that will be pushing high data volumes into Clowder:
* LemnaTec indoor system at Danforth (running)
* LemnaTec outdoor system at Maricopa (in progress)

For typical workflows, the following steps are sufficient to push data into Clowder in an organized fashion:

1. Create a collection to hold relevant datasets (optional)
    ```POST /api/collections``` _provide a name; returns collection ID_  
    
2. Create a dataset to hold relevant files and add it to the collection
    ```POST /api/datasets/createempty``` _provide a name; returns dataset ID_  
    ```POST /api/collections/<collection id>/datasets/<dataset id>```  
    
3. Upload files and metadata to dataset
    ```POST /api/datasets/uploadToDataset/<dataset id>``` _provide file(s) and metadata_  

An extensive API reference can be found [here](https://terraref.ncsa.illinois.edu/clowder/assets/docs/api/index.html).

### Uploading Data Using Globus

Some files, e.g. those transferred via Globus, will be moved to the server without triggering Clowder's normal upload paths. These must be transmitted in a certain way to ensure proper handling.

1. Log into Globus and click 'Transfer Files'.
2. Select your source endpoint, and [Terraref](https://www.globus.org/app/endpoints/c50eec62-ea1a-11e5-97d6-22000b9da45e/overview) as the destination. You need to contact NCSA to ensure you have the necessary credentials and folder space to utilize Globus - unrecognized Globus accounts will not be trusted.
3. Transfer your files. You will receive a **Task ID** when the transfer starts.
4. Send this Task ID and requisite information about the transfer to the **TERRAREF Globus Monitor API** as a JSON object:
  ```json
    { "user": <globus_username>
      "globus_id": <Task ID>
      "contents": {
        <dataset1>: {
          <filename1>: {
            "name": <filename1>,
            "md": <file_metadata1>
          },
          <filename2>: {"name": ..., "md": {...}},
          <filename3>: {...},
          ...
        },
        <dataset2>: {
          <filename4>: {...},
          ...
        },
        ...
        }
      }
    }
  ```
  In addition to username and Task ID, you must also send a "contents" object containing each dataset that should be created in Clowder, and the files that belong to that dataset. This allows Clowder to verify it has handled every file in the Globus task.
5. The JSON object is sent to the API via an HTTP request:
 ```POST 141.142.168.72:5454/tasks```
 For example, with cURL this would be done with:
 ```curl -X POST -u <globus_username>:<globus_password> -d <json_object> 141.142.168.72:5454/tasks```

In this way Clowder indexes a pointer to the file on disk rather than making a new copy of the file; thus the file will still be accessible via Globus, FTP, or other methods directed at the filesystem.

### References 

* **Source code**: The source code is available as a collection of Git repositories.
* Tutorial
 * [slides](https://onedrive.live.com/embed?cid=62A7CDC1353EF6B0&resid=62A7CDC1353EF6B0%211988&authkey=ALJbJ7PqsDaYAxU&em=2&wdAr=1.7777777777777777) 
 * [video](https://www.youtube.com/embed/lP3vqh6HLG4).
* [pyClowder](https://opensource.ncsa.illinois.edu/bitbucket/projects/CATS/repos/pyclowder/browse) is designed for this purpose.
* [Development in Windows](https://opensource.ncsa.illinois.edu/confluence/display/CATS/Deploying+Windows+Extractors)
* [Using Clowder via National Data Service interface](https://www.youtube.com/embed/dCNYEl3ld0s)


* Contacts: Max Burnette via [email, phone](http://www.ncsa.illinois.edu/assets/php/directory/contact.php?contact=mburnet2), on GitHub, or on our [Slack Channel](https://terra-ref.slack.com/)

**Examples** (from the [git repository](https://opensource.ncsa.illinois.edu/bitbucket/projects/CATS))  
* **extractors-core** includes basic extractors, such as image thumbnail extraction.
* **extractors-dbpedia** uses named-entity recognition and [DBpedia](http://wiki.dbpedia.org/) to extract information from text files
* **extractors-plantcv** invokes appropriate [PlantCV](http://plantcv.danforthcenter.org/) image analysis tools to generate output images and data from uploaded images ([read more about this extractor here](http://opensource.ncsa.illinois.edu/bitbucket/projects/CATS/repos/extractors-plantcv/browse))


